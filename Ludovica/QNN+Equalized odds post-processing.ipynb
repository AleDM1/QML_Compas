{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b43f67-e466-47a4-b3b7-a23b2e44aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installa pacchetti se non sono gi√† presenti\n",
    "!pip install -q pandas numpy scikit-learn matplotlib fairlearn qiskit qiskit-machine-learning torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e5af6f-51c6-44f4-b644-769f41985f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ffceec-43af-49d0-8f85-893ce1761728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignora warning di precisione indefinita\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. Carica il dataset COMPAS filtrando per Caucasian e African-American\n",
    "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df[df['race'].isin(['Caucasian', 'African-American'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff2773f-509e-41d5-b123-1ed83122e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crea la variabile sensibile 'race_binary': 1 = African-American, 0 = Caucasian\n",
    "df['race_binary'] = (df['race'] == 'African-American').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d87a58-e1ef-460a-8f4b-93920c950070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6932\n",
      "Epoch 2, Loss: 0.6623\n",
      "Epoch 3, Loss: 0.6750\n",
      "Epoch 4, Loss: 0.6607\n",
      "Epoch 5, Loss: 0.7244\n",
      "Epoch 6, Loss: 0.7004\n",
      "Epoch 7, Loss: 0.6667\n",
      "Epoch 8, Loss: 0.6981\n",
      "Epoch 9, Loss: 0.6522\n",
      "Epoch 10, Loss: 0.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages/fairlearn/postprocessing/_interpolated_thresholder.py:154: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 7.49400000e-02 7.49400000e-02 9.99307350e-01\n",
      " 7.49400000e-02 7.49400000e-02 9.99307350e-01 7.49400000e-02\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 7.49400000e-02 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 7.49400000e-02 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 7.49400000e-02 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 7.49400000e-02 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 7.49400000e-02 9.99307350e-01\n",
      " 9.99307350e-01 7.49400000e-02 7.49400000e-02 7.49400000e-02\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 7.49400000e-02 9.99307350e-01\n",
      " 9.99307350e-01 7.49400000e-02 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 9.99307350e-01 9.99307350e-01\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 9.99307350e-01 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 9.99307350e-01 7.49400000e-02\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04]' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  positive_probs[sensitive_feature_vector == a] = interpolated_predictions[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Accuracy       AUC        F1  Precision    Recall       MCC  \\\n",
      "QNN Before  0.618970  0.675406  0.422350   0.719888  0.298837  0.249167   \n",
      "QNN After   0.650407  0.675406  0.563895   0.673667  0.484884  0.295622   \n",
      "MLP Before  0.679675  0.731260  0.627599   0.685007  0.579070  0.353823   \n",
      "MLP After   0.672087  0.731260  0.615873   0.678322  0.563953  0.338362   \n",
      "\n",
      "                 EOD       AOD        DI       SPR  \n",
      "QNN Before  0.141548  0.106637  1.987752  0.119563  \n",
      "QNN After   0.007500  0.002856  1.084715  0.027034  \n",
      "MLP Before  0.239167  0.200759  1.877780  0.225784  \n",
      "MLP After  -0.004167 -0.002692  1.071739  0.026643  \n"
     ]
    }
   ],
   "source": [
    "# 3. Target: recidiva entro 2 anni\n",
    "y = df['two_year_recid'].astype(int).values\n",
    "sensitive_attr = df['race_binary'].values\n",
    "\n",
    "# 4. Selezione delle feature\n",
    "features = ['age', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'c_charge_degree']\n",
    "df = df.copy()\n",
    "df['c_charge_degree'] = df['c_charge_degree'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "X = df[features].fillna(0)\n",
    "\n",
    "# 5. Suddivisione in train/test\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X, y, sensitive_attr, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "# 6. Normalizzazione\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 7. QNN: costruzione circuito parametrico\n",
    "num_qubits = X.shape[1]\n",
    "x_params = ParameterVector(\"x\", num_qubits)\n",
    "y_params = ParameterVector(\"y\", num_qubits)\n",
    "qc = QuantumCircuit(num_qubits)\n",
    "for i in range(num_qubits):\n",
    "    qc.ry(x_params[i], i)\n",
    "\n",
    "# Aggiunta di entanglement e profondit√†\n",
    "for _ in range(2):\n",
    "    for i in range(num_qubits - 1):\n",
    "        qc.cz(i, i+1)\n",
    "    for i in range(num_qubits):\n",
    "        qc.rx(y_params[i], i)\n",
    "        qc.ry(y_params[i], i)\n",
    "\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qc,\n",
    "    input_params=x_params,\n",
    "    weight_params=y_params\n",
    ")\n",
    "\n",
    "model = TorchConnector(qnn)\n",
    "\n",
    "# 8. PyTorch training con mini-batch e pi√π epoche\n",
    "class QuantumClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qnn = model\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.qnn(x))\n",
    "\n",
    "qc_model = QuantumClassifier()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(qc_model.parameters(), lr=0.001)\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train * 1.0, dtype=torch.float32).view(-1, 1)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#aumentare le epoche a 200\n",
    "for epoch in range(10):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = qc_model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 9. Predizioni su test set (QNN)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_scores_qnn = qc_model(X_test_torch).detach().numpy().flatten()\n",
    "y_pred_qnn = (y_scores_qnn >= 0.5).astype(int)\n",
    "\n",
    "# 10. Dummy model per Fairlearn (QNN)\n",
    "class QuantumProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - np.array(y_scores_qnn), np.array(y_scores_qnn)]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "X_dummy = np.zeros((len(y_test), 1))\n",
    "postproc_qnn = ThresholdOptimizer(\n",
    "    estimator=QuantumProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "postproc_qnn.fit(X=X_dummy, y=y_test, sensitive_features=sens_test)\n",
    "y_pred_qnn_fair = postproc_qnn.predict(X=X_dummy, sensitive_features=sens_test)\n",
    "\n",
    "# 11. Classico MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_scores_mlp = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "\n",
    "# 12. Equalized Odds post-processing (MLP)\n",
    "class MLPProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - y_scores_mlp, y_scores_mlp]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "postproc_mlp = ThresholdOptimizer(\n",
    "    estimator=MLPProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "postproc_mlp.fit(X=X_dummy, y=y_test, sensitive_features=sens_test)\n",
    "y_pred_mlp_fair = postproc_mlp.predict(X=X_dummy, sensitive_features=sens_test)\n",
    "\n",
    "# 13. Metriche\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "def full_metrics(y_true, y_pred, y_score, sensitive_attr):\n",
    "    s = np.array(sensitive_attr)\n",
    "    yt, yp = np.array(y_true), np.array(y_pred)\n",
    "    priv, unpriv = s == 0, s == 1\n",
    "\n",
    "    def tpr(y, yhat): return np.mean((yhat == 1) & (y == 1)) / max(np.mean(y == 1), 1e-6)\n",
    "    def fpr(y, yhat): return np.mean((yhat == 1) & (y == 0)) / max(np.mean(y == 0), 1e-6)\n",
    "    sr_priv, sr_unpriv = np.mean(yp[priv]), np.mean(yp[unpriv])\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(yt, yp),\n",
    "        'AUC': roc_auc_score(yt, y_score),\n",
    "        'F1': f1_score(yt, yp, zero_division=0),\n",
    "        'Precision': precision_score(yt, yp, zero_division=0),\n",
    "        'Recall': recall_score(yt, yp, zero_division=0),\n",
    "        'MCC': matthews_corrcoef(yt, yp),\n",
    "        'EOD': tpr(yt[unpriv], yp[unpriv]) - tpr(yt[priv], yp[priv]),\n",
    "        'AOD': 0.5 * ((fpr(yt[unpriv], yp[unpriv]) - fpr(yt[priv], yp[priv])) +\n",
    "                      (tpr(yt[unpriv], yp[unpriv]) - tpr(yt[priv], yp[priv]))),\n",
    "        'DI': sr_unpriv / sr_priv if sr_priv > 0 else np.nan,\n",
    "        'SPR': sr_unpriv - sr_priv\n",
    "    }\n",
    "\n",
    "metrics_qnn_before = full_metrics(y_test, y_pred_qnn, y_scores_qnn, sens_test)\n",
    "metrics_qnn_after = full_metrics(y_test, y_pred_qnn_fair, y_scores_qnn, sens_test)\n",
    "metrics_mlp_before = full_metrics(y_test, y_pred_mlp, y_scores_mlp, sens_test)\n",
    "metrics_mlp_after = full_metrics(y_test, y_pred_mlp_fair, y_scores_mlp, sens_test)\n",
    "\n",
    "# 14. Visualizzazione\n",
    "metrics_df = pd.DataFrame([\n",
    "    metrics_qnn_before,\n",
    "    metrics_qnn_after,\n",
    "    metrics_mlp_before,\n",
    "    metrics_mlp_after\n",
    "], index=[\"QNN Before\", \"QNN After\", \"MLP Before\", \"MLP After\"])\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7316ee9-b3a1-4b54-8b34-66b36b4e25b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qiskit-env)",
   "language": "python",
   "name": "qiskit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
