{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b43f67-e466-47a4-b3b7-a23b2e44aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installa pacchetti se non sono già presenti\n",
    "!pip install -q pandas numpy scikit-learn matplotlib fairlearn qiskit qiskit-machine-learning torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e5af6f-51c6-44f4-b644-769f41985f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ffceec-43af-49d0-8f85-893ce1761728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignora warning di precisione indefinita\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. Carica il dataset COMPAS filtrando per Caucasian e African-American\n",
    "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df[df['race'].isin(['Caucasian', 'African-American'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff2773f-509e-41d5-b123-1ed83122e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crea la variabile sensibile 'race_binary': 1 = African-American, 0 = Caucasian\n",
    "df['race_binary'] = (df['race'] == 'African-American').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d87a58-e1ef-460a-8f4b-93920c950070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6932\n",
      "Epoch 2, Loss: 0.6623\n",
      "Epoch 3, Loss: 0.6750\n",
      "Epoch 4, Loss: 0.6607\n",
      "Epoch 5, Loss: 0.7244\n",
      "Epoch 6, Loss: 0.7004\n",
      "Epoch 7, Loss: 0.6667\n",
      "Epoch 8, Loss: 0.6981\n",
      "Epoch 9, Loss: 0.6522\n",
      "Epoch 10, Loss: 0.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages/fairlearn/postprocessing/_interpolated_thresholder.py:154: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 7.49400000e-02 7.49400000e-02 9.99307350e-01\n",
      " 7.49400000e-02 7.49400000e-02 9.99307350e-01 7.49400000e-02\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 7.49400000e-02 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 7.49400000e-02 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 7.49400000e-02 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 7.49400000e-02 1.81908095e-04\n",
      " 9.99307350e-01 7.49400000e-02 7.49400000e-02 9.99307350e-01\n",
      " 9.99307350e-01 7.49400000e-02 7.49400000e-02 7.49400000e-02\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 7.49400000e-02 9.99307350e-01\n",
      " 9.99307350e-01 7.49400000e-02 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 9.99307350e-01 9.99307350e-01\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 7.49400000e-02\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 7.49400000e-02\n",
      " 9.99307350e-01 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 9.99307350e-01 9.99307350e-01 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 7.49400000e-02 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 9.99307350e-01\n",
      " 1.81908095e-04 1.81908095e-04 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 7.49400000e-02 1.81908095e-04\n",
      " 1.81908095e-04 9.99307350e-01 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 7.49400000e-02 9.99307350e-01 9.99307350e-01 7.49400000e-02\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 1.81908095e-04 7.49400000e-02 1.81908095e-04 7.49400000e-02\n",
      " 1.81908095e-04 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 9.99307350e-01 1.81908095e-04\n",
      " 9.99307350e-01 9.99307350e-01 1.81908095e-04 1.81908095e-04\n",
      " 9.99307350e-01 1.81908095e-04 1.81908095e-04 9.99307350e-01\n",
      " 1.81908095e-04 9.99307350e-01 1.81908095e-04]' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  positive_probs[sensitive_feature_vector == a] = interpolated_predictions[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Accuracy       AUC        F1  Precision    Recall       MCC  \\\n",
      "QNN Before  0.618970  0.675406  0.422350   0.719888  0.298837  0.249167   \n",
      "QNN After   0.650407  0.675406  0.563895   0.673667  0.484884  0.295622   \n",
      "MLP Before  0.679675  0.731260  0.627599   0.685007  0.579070  0.353823   \n",
      "MLP After   0.672087  0.731260  0.615873   0.678322  0.563953  0.338362   \n",
      "\n",
      "                 EOD       AOD        DI       SPR  \n",
      "QNN Before  0.141548  0.106637  1.987752  0.119563  \n",
      "QNN After   0.007500  0.002856  1.084715  0.027034  \n",
      "MLP Before  0.239167  0.200759  1.877780  0.225784  \n",
      "MLP After  -0.004167 -0.002692  1.071739  0.026643  \n"
     ]
    }
   ],
   "source": [
    "# 3. Target: recidiva entro 2 anni\n",
    "y = df['two_year_recid'].astype(int).values\n",
    "sensitive_attr = df['race_binary'].values\n",
    "\n",
    "# 4. Selezione delle feature\n",
    "features = ['age', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'c_charge_degree']\n",
    "df = df.copy()\n",
    "df['c_charge_degree'] = df['c_charge_degree'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "X = df[features].fillna(0)\n",
    "\n",
    "# 5. Suddivisione in train/test\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X, y, sensitive_attr, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "# 6. Normalizzazione\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 7. QNN: costruzione circu# ... (tutto il codice precedente per COMPAS rimane invariato)\n",
    "\n",
    "# 6. Adversarial Debiasing (Quantum Predictor + Classico Adversary)\n",
    "\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class QAdvModel(nn.Module):\n",
    "    def __init__(self, q_model, adv_model, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.q_model = q_model\n",
    "        self.adv_model = adv_model\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        q_out = self.q_model(x)\n",
    "        adv_out = self.adv_model(q_out.detach())\n",
    "        return q_out, adv_out\n",
    "\n",
    "q_model = QuantumClassifier()\n",
    "adversary = Adversary(1)  # 1 output dalla QNN\n",
    "q_adv = QAdvModel(q_model, adversary, alpha=1.0)\n",
    "\n",
    "opt_pred = torch.optim.AdamW(q_adv.q_model.parameters(), lr=0.001)\n",
    "opt_adv = torch.optim.AdamW(q_adv.adv_model.parameters(), lr=0.001)\n",
    "\n",
    "sens_train_torch = torch.tensor(sens_train * 1.0, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch, s_batch in zip(X_train_torch, y_train_torch, sens_train_torch):\n",
    "        x_batch = x_batch.view(1, -1)\n",
    "        y_batch = y_batch.view(1, -1)\n",
    "        s_batch = s_batch.view(1, -1)\n",
    "\n",
    "        opt_pred.zero_grad()\n",
    "        opt_adv.zero_grad()\n",
    "\n",
    "        pred_out, adv_out = q_adv(x_batch)\n",
    "\n",
    "        pred_loss = loss_fn(pred_out, y_batch)\n",
    "        adv_loss = loss_fn(adv_out, s_batch)\n",
    "        loss = pred_loss - q_adv.alpha * adv_loss\n",
    "\n",
    "        loss.backward()\n",
    "        opt_pred.step()\n",
    "        opt_adv.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[AdvEpoch {epoch+1}] Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Valutazione del modello adversarial\n",
    "qadv_scores = q_model(X_test_torch).detach().numpy().flatten()\n",
    "qadv_pred = (qadv_scores >= 0.5).astype(int)\n",
    "\n",
    "metrics_qadv = full_metrics(y_test, qadv_pred, qadv_scores, sens_test)\n",
    "metrics_df.loc['QNN Adversarial'] = metrics_qadv\n",
    "\n",
    "print(\"\\n=== Confronto con QNN Adversarial Debiasing ===\")\n",
    "print(metrics_df)\n",
    "ito parametrico\n",
    "num_qubits = X.shape[1]\n",
    "x_params = ParameterVector(\"x\", num_qubits)\n",
    "y_params = ParameterVector(\"y\", num_qubits)\n",
    "qc = QuantumCircuit(num_qubits)\n",
    "for i in range(num_qubits):\n",
    "    qc.ry(x_params[i], i)\n",
    "\n",
    "# Aggiunta di entanglement e profondità\n",
    "for _ in range(2):\n",
    "    for i in range(num_qubits - 1):\n",
    "        qc.cz(i, i+1)\n",
    "    for i in range(num_qubits):\n",
    "        qc.rx(y_params[i], i)\n",
    "        qc.ry(y_params[i], i)\n",
    "\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qc,\n",
    "    input_params=x_params,\n",
    "    weight_params=y_params\n",
    ")\n",
    "\n",
    "model = TorchConnector(qnn)\n",
    "\n",
    "# 8. PyTorch training con mini-batch e più epoche\n",
    "class QuantumClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qnn = model\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.qnn(x))\n",
    "\n",
    "qc_model = QuantumClassifier()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(qc_model.parameters(), lr=0.001)\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train * 1.0, dtype=torch.float32).view(-1, 1)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#aumentare le epoche a 200\n",
    "for epoch in range(10):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = qc_model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 9. Predizioni su test set (QNN)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_scores_qnn = qc_model(X_test_torch).detach().numpy().flatten()\n",
    "y_pred_qnn = (y_scores_qnn >= 0.5).astype(int)\n",
    "\n",
    "# 10. Dummy model per Fairlearn (QNN)\n",
    "class QuantumProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - np.array(y_scores_qnn), np.array(y_scores_qnn)]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "X_dummy = np.zeros((len(y_test), 1))\n",
    "postproc_qnn = ThresholdOptimizer(\n",
    "    estimator=QuantumProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "postproc_qnn.fit(X=X_dummy, y=y_test, sensitive_features=sens_test)\n",
    "y_pred_qnn_fair = postproc_qnn.predict(X=X_dummy, sensitive_features=sens_test)\n",
    "\n",
    "# 11. Classico MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_scores_mlp = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "\n",
    "# 12. Equalized Odds post-processing (MLP)\n",
    "class MLPProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - y_scores_mlp, y_scores_mlp]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "postproc_mlp = ThresholdOptimizer(\n",
    "    estimator=MLPProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "postproc_mlp.fit(X=X_dummy, y=y_test, sensitive_features=sens_test)\n",
    "y_pred_mlp_fair = postproc_mlp.predict(X=X_dummy, sensitive_features=sens_test)\n",
    "\n",
    "# 13. Metriche\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "def full_metrics(y_true, y_pred, y_score, sensitive_attr):\n",
    "    s = np.array(sensitive_attr)\n",
    "    yt, yp = np.array(y_true), np.array(y_pred)\n",
    "    priv, unpriv = s == 0, s == 1\n",
    "\n",
    "    def tpr(y, yhat): return np.mean((yhat == 1) & (y == 1)) / max(np.mean(y == 1), 1e-6)\n",
    "    def fpr(y, yhat): return np.mean((yhat == 1) & (y == 0)) / max(np.mean(y == 0), 1e-6)\n",
    "    sr_priv, sr_unpriv = np.mean(yp[priv]), np.mean(yp[unpriv])\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(yt, yp),\n",
    "        'AUC': roc_auc_score(yt, y_score),\n",
    "        'F1': f1_score(yt, yp, zero_division=0),\n",
    "        'Precision': precision_score(yt, yp, zero_division=0),\n",
    "        'Recall': recall_score(yt, yp, zero_division=0),\n",
    "        'MCC': matthews_corrcoef(yt, yp),\n",
    "        'EOD': tpr(yt[unpriv], yp[unpriv]) - tpr(yt[priv], yp[priv]),\n",
    "        'AOD': 0.5 * ((fpr(yt[unpriv], yp[unpriv]) - fpr(yt[priv], yp[priv])) +\n",
    "                      (tpr(yt[unpriv], yp[unpriv]) - tpr(yt[priv], yp[priv]))),\n",
    "        'DI': sr_unpriv / sr_priv if sr_priv > 0 else np.nan,\n",
    "        'SPR': sr_unpriv - sr_priv\n",
    "    }\n",
    "\n",
    "metrics_qnn_before = full_metrics(y_test, y_pred_qnn, y_scores_qnn, sens_test)\n",
    "metrics_qnn_after = full_metrics(y_test, y_pred_qnn_fair, y_scores_qnn, sens_test)\n",
    "metrics_mlp_before = full_metrics(y_test, y_pred_mlp, y_scores_mlp, sens_test)\n",
    "metrics_mlp_after = full_metrics(y_test, y_pred_mlp_fair, y_scores_mlp, sens_test)\n",
    "\n",
    "# 14. Visualizzazione\n",
    "metrics_df = pd.DataFrame([\n",
    "    metrics_qnn_before,\n",
    "    metrics_qnn_after,\n",
    "    metrics_mlp_before,\n",
    "    metrics_mlp_after\n",
    "], index=[\"QNN Before\", \"QNN After\", \"MLP Before\", \"MLP After\"])\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7316ee9-b3a1-4b54-8b34-66b36b4e25b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7074\n",
      "Epoch 2, Loss: 0.7092\n",
      "Epoch 3, Loss: 0.7409\n",
      "Epoch 4, Loss: 0.7052\n",
      "Epoch 5, Loss: 0.7149\n",
      "Epoch 6, Loss: 0.6863\n",
      "Epoch 7, Loss: 0.6740\n",
      "Epoch 8, Loss: 0.6471\n",
      "Epoch 9, Loss: 0.6797\n",
      "Epoch 10, Loss: 0.6693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages/fairlearn/postprocessing/_interpolated_thresholder.py:154: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.         1.         0.         0.00147239 0.00147239 0.\n",
      " 0.         0.         0.00147239 0.00147239 0.         0.00147239\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00147239 0.         0.00147239 0.         0.         0.00147239\n",
      " 0.         0.         0.00147239 0.00147239 0.00147239 0.\n",
      " 0.         0.00147239 0.00147239 0.         1.         0.\n",
      " 1.         0.         0.         0.         1.         0.00147239\n",
      " 1.         1.         1.         1.         1.         0.00147239\n",
      " 1.         1.         0.         0.         0.00147239 0.\n",
      " 0.00147239 0.         0.00147239 0.00147239 1.         0.00147239\n",
      " 1.         0.00147239 0.         0.00147239 1.         0.\n",
      " 0.         0.         0.00147239 0.         0.00147239 0.00147239\n",
      " 0.         1.         1.         1.         0.00147239 0.\n",
      " 0.00147239 0.00147239 0.00147239 0.         0.00147239 1.\n",
      " 0.         0.00147239 0.         0.00147239 0.         0.00147239\n",
      " 1.         0.         0.         0.         0.00147239 1.\n",
      " 0.00147239 0.         0.00147239 0.         0.00147239 0.\n",
      " 0.00147239 0.         0.00147239 0.00147239 1.         1.\n",
      " 0.         0.00147239 0.         0.         0.00147239 1.\n",
      " 0.00147239 0.         0.         0.         0.         0.00147239\n",
      " 0.00147239 0.00147239 0.00147239 1.         1.         0.\n",
      " 0.00147239 0.         0.00147239 0.         1.         0.00147239\n",
      " 0.         1.         0.         1.         0.00147239 0.\n",
      " 1.         0.         1.         1.         0.00147239 0.\n",
      " 0.00147239 0.00147239 0.00147239 0.00147239 0.         0.00147239\n",
      " 0.00147239 0.         1.         0.         0.         0.00147239\n",
      " 0.         0.         0.00147239 1.         0.00147239 0.\n",
      " 0.00147239 0.         0.00147239 0.00147239 0.         0.00147239\n",
      " 0.00147239 0.         0.00147239 0.00147239 0.         0.\n",
      " 1.         0.00147239 0.00147239 0.00147239 0.00147239 0.00147239\n",
      " 0.         0.00147239 0.00147239 0.00147239 0.00147239 0.\n",
      " 1.         0.         0.         0.         0.00147239 0.00147239\n",
      " 1.         0.         0.00147239 0.00147239 0.00147239 0.00147239\n",
      " 0.00147239 0.00147239 1.         0.         0.         0.\n",
      " 0.00147239 0.         0.         0.00147239 0.         0.00147239\n",
      " 0.         0.         0.         0.00147239 0.00147239 0.00147239\n",
      " 0.         0.00147239 0.         0.         1.         1.\n",
      " 0.00147239 1.         0.         0.         0.         0.00147239\n",
      " 1.         0.00147239 0.         0.         0.         0.00147239\n",
      " 1.         0.00147239 0.         0.00147239 0.00147239 0.00147239\n",
      " 0.00147239 1.         0.         0.00147239 0.         0.\n",
      " 0.00147239 0.         0.         0.         0.00147239 0.00147239\n",
      " 0.         0.         0.00147239 0.         0.00147239 0.\n",
      " 0.00147239 0.         0.         0.00147239 0.00147239 0.\n",
      " 0.00147239 0.         0.         0.00147239 1.         1.\n",
      " 0.         0.00147239 0.         0.00147239 0.00147239 0.\n",
      " 1.         0.         1.         0.         0.00147239 1.\n",
      " 0.         0.00147239 0.00147239 0.00147239 0.         0.00147239\n",
      " 0.00147239 1.         0.         0.00147239 0.         0.00147239\n",
      " 0.         0.00147239 0.         0.00147239 1.         0.\n",
      " 0.00147239 0.         0.00147239 0.00147239 0.         0.\n",
      " 0.         1.         0.         0.         1.         0.00147239\n",
      " 0.00147239 0.         0.         0.00147239 0.00147239 0.\n",
      " 0.         0.         0.00147239 0.         0.00147239 0.00147239\n",
      " 0.00147239 0.00147239 1.         0.00147239 1.         0.00147239\n",
      " 0.         0.00147239 1.         0.         0.         0.00147239\n",
      " 1.         1.         0.         0.00147239 0.         0.00147239\n",
      " 0.00147239 0.00147239 0.         1.         0.         0.00147239\n",
      " 0.         0.         0.00147239 1.         0.00147239 0.00147239\n",
      " 0.         1.         1.         0.00147239 0.         1.\n",
      " 0.00147239 0.00147239 1.         0.00147239 0.         0.00147239\n",
      " 0.         0.00147239 1.         0.         0.         0.00147239\n",
      " 0.00147239 0.         1.         0.00147239 0.00147239 0.00147239\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.00147239 0.00147239 0.         0.00147239 0.\n",
      " 1.         0.         1.         0.00147239 1.         0.00147239\n",
      " 0.00147239 1.         0.         0.         0.00147239 0.\n",
      " 0.00147239 0.00147239 1.         0.         0.         0.\n",
      " 0.00147239 0.         0.00147239 0.         1.         0.\n",
      " 0.00147239 0.00147239 0.         0.         0.         0.00147239\n",
      " 1.         1.         0.00147239 0.         1.         0.00147239\n",
      " 1.         1.         0.00147239 0.00147239 0.00147239 1.\n",
      " 0.00147239 0.         1.         0.         0.         0.\n",
      " 0.00147239 0.         0.         0.00147239 0.00147239 0.00147239\n",
      " 1.         0.00147239 0.         1.         0.         0.00147239\n",
      " 0.         0.00147239 0.00147239 0.         0.         0.\n",
      " 0.00147239 0.         0.00147239 0.00147239 0.00147239 0.00147239\n",
      " 0.         0.00147239 0.00147239 1.         1.         0.\n",
      " 0.00147239 0.00147239 1.         0.         0.         0.00147239\n",
      " 0.00147239 1.         0.00147239 0.         0.00147239 0.\n",
      " 0.         0.         0.         1.         0.00147239 0.00147239\n",
      " 0.         0.         0.00147239 0.         1.         0.00147239\n",
      " 0.00147239 1.         1.         0.         0.         0.\n",
      " 0.         1.         0.00147239 1.         0.         0.\n",
      " 1.         0.         0.         0.         1.         0.\n",
      " 0.00147239 0.00147239 1.         0.00147239 0.         1.\n",
      " 1.         1.         0.         0.         0.         0.\n",
      " 0.00147239 1.         0.00147239 0.00147239 0.         0.\n",
      " 0.         0.         0.         0.00147239 0.         1.\n",
      " 0.00147239 1.         0.00147239 0.         0.         0.\n",
      " 0.00147239 0.00147239 0.         0.00147239 0.00147239 0.\n",
      " 0.00147239 0.00147239 0.00147239 0.         0.00147239 1.\n",
      " 1.         0.00147239 1.         1.         1.         0.00147239\n",
      " 0.         0.00147239 0.00147239 0.         0.00147239 0.00147239\n",
      " 1.         0.00147239 1.         0.         1.         0.00147239\n",
      " 0.         0.         0.00147239 0.00147239 1.         0.00147239\n",
      " 0.00147239 0.         1.         1.         0.         1.\n",
      " 1.         0.         0.         0.00147239 0.         1.\n",
      " 0.00147239 0.00147239 0.00147239 0.00147239 0.         0.00147239\n",
      " 1.         0.         0.         0.00147239 0.00147239 0.\n",
      " 0.         0.         0.         1.         0.         0.00147239\n",
      " 0.         0.         1.         0.         0.         0.00147239\n",
      " 0.00147239 0.         0.         1.         0.         0.00147239\n",
      " 0.         1.         0.00147239 1.         1.         0.\n",
      " 0.         0.         0.         0.         0.00147239 0.\n",
      " 0.00147239 1.         0.         0.00147239 0.         0.00147239\n",
      " 0.         0.         0.00147239 0.         0.         0.\n",
      " 0.         0.         0.00147239 0.00147239 0.00147239 0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.00147239 0.00147239 0.         0.         0.00147239 0.00147239\n",
      " 0.         0.00147239 0.00147239 0.         0.         0.\n",
      " 0.00147239 1.         0.         0.         1.         0.\n",
      " 0.         0.00147239 0.         0.         0.         0.00147239\n",
      " 1.         0.         0.         1.         1.         0.\n",
      " 0.         0.         0.00147239 0.00147239 0.00147239 1.\n",
      " 1.         0.00147239 0.         0.00147239 0.         0.\n",
      " 0.00147239 0.         1.         0.         0.         1.\n",
      " 0.         0.00147239 0.00147239 0.         0.00147239 0.\n",
      " 1.         0.00147239 0.00147239 0.         1.         0.\n",
      " 0.00147239 0.         1.         1.         0.         0.\n",
      " 1.         0.         0.         1.         0.         1.\n",
      " 0.        ]' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  positive_probs[sensitive_feature_vector == a] = interpolated_predictions[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Accuracy       AUC        F1  Precision    Recall       MCC  \\\n",
      "QNN Before     0.600000  0.651300  0.584927   0.566449  0.604651  0.200132   \n",
      "QNN After      0.608672  0.651300  0.414911   0.684492  0.297674  0.220722   \n",
      "MLP Before     0.679675  0.731260  0.627599   0.685007  0.579070  0.353823   \n",
      "MLP After      0.670461  0.731260  0.616646   0.673554  0.568605  0.334928   \n",
      "LogReg Before  0.675881  0.726159  0.620558   0.682961  0.568605  0.346155   \n",
      "LogReg After   0.673713  0.726159  0.607562   0.691395  0.541860  0.342597   \n",
      "\n",
      "                    EOD       AOD        DI       SPR  \n",
      "QNN Before     0.196548  0.155187  1.422462  0.167358  \n",
      "QNN After      0.001548  0.001091  1.086877  0.016730  \n",
      "MLP Before     0.239167  0.200759  1.877780  0.225784  \n",
      "MLP After      0.002976  0.007152  1.098231  0.036482  \n",
      "LogReg Before  0.243571  0.207371  1.936355  0.231835  \n",
      "LogReg After   0.002857 -0.006491  1.061766  0.021750  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. Caricamento e pre-processing COMPAS\n",
    "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df[df['race'].isin(['Caucasian', 'African-American'])].copy()\n",
    "df['race_binary'] = (df['race'] == 'African-American').astype(int)\n",
    "df['c_charge_degree'] = df['c_charge_degree'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "\n",
    "features = ['age', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'c_charge_degree']\n",
    "X = df[features].fillna(0)\n",
    "y = df['two_year_recid'].astype(int).values\n",
    "sensitive_attr = df['race_binary'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X, y, sensitive_attr, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. QNN come regressione logistica quantistica\n",
    "num_qubits = X.shape[1]\n",
    "x_params = ParameterVector(\"x\", num_qubits)\n",
    "y_params = ParameterVector(\"y\", num_qubits)\n",
    "qc = QuantumCircuit(num_qubits)\n",
    "for i in range(num_qubits):\n",
    "    qc.ry(x_params[i], i)\n",
    "for _ in range(2):\n",
    "    for i in range(num_qubits - 1):\n",
    "        qc.cz(i, i+1)\n",
    "    for i in range(num_qubits):\n",
    "        qc.rx(y_params[i], i)\n",
    "        qc.ry(y_params[i], i)\n",
    "\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qc,\n",
    "    input_params=x_params,\n",
    "    weight_params=y_params\n",
    ")\n",
    "\n",
    "model = TorchConnector(qnn)\n",
    "\n",
    "class QuantumClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qnn = model\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.qnn(x))\n",
    "\n",
    "qc_model = QuantumClassifier()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(qc_model.parameters(), lr=0.001)\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train * 1.0, dtype=torch.float32).view(-1, 1)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = qc_model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_scores_qnn = qc_model(X_test_torch).detach().numpy().flatten()\n",
    "y_pred_qnn = (y_scores_qnn >= 0.5).astype(int)\n",
    "\n",
    "class QuantumProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - np.array(y_scores_qnn), np.array(y_scores_qnn)]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "X_dummy = np.zeros((len(y_test), 1))\n",
    "postproc_qnn = ThresholdOptimizer(\n",
    "    estimator=QuantumProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "postproc_qnn.fit(X=X_dummy, y=y_test, sensitive_features=sens_test)\n",
    "y_pred_qnn_fair = postproc_qnn.predict(X=X_dummy, sensitive_features=sens_test)\n",
    "\n",
    "# 3. MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_scores_mlp = mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "\n",
    "class MLPProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - y_scores_mlp, y_scores_mlp]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "postproc_mlp = ThresholdOptimizer(\n",
    "    estimator=MLPProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "postproc_mlp.fit(X=X_dummy, y=y_test, sensitive_features=sens_test)\n",
    "y_pred_mlp_fair = postproc_mlp.predict(X=X_dummy, sensitive_features=sens_test)\n",
    "\n",
    "# Nota: la QNN nel nostro modello agisce come una regressione logistica quantistica\n",
    "# perché produce una probabilità tramite funzione sigmoide e viene ottimizzata\n",
    "# con Binary Cross Entropy Loss (BCELoss), esattamente come una logistic regression classica.\n",
    "\n",
    "# 4. Logistic Regression classica\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_scores_logreg = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_logreg = logreg.predict(X_test_scaled)\n",
    "\n",
    "class LogRegProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - y_scores_logreg, y_scores_logreg]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "postproc_logreg = ThresholdOptimizer(\n",
    "    estimator=LogRegProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "postproc_logreg.fit(X=X_dummy, y=y_test, sensitive_features=sens_test)\n",
    "y_pred_logreg_fair = postproc_logreg.predict(X=X_dummy, sensitive_features=sens_test)\n",
    "\n",
    "# 5. Metriche\n",
    "\n",
    "def full_metrics(y_true, y_pred, y_score, sensitive_attr):\n",
    "    s = np.array(sensitive_attr)\n",
    "    yt, yp = np.array(y_true), np.array(y_pred)\n",
    "    priv, unpriv = s == 0, s == 1\n",
    "\n",
    "    def tpr(y, yhat): return np.mean((yhat == 1) & (y == 1)) / max(np.mean(y == 1), 1e-6)\n",
    "    def fpr(y, yhat): return np.mean((yhat == 1) & (y == 0)) / max(np.mean(y == 0), 1e-6)\n",
    "    sr_priv, sr_unpriv = np.mean(yp[priv]), np.mean(yp[unpriv])\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(yt, yp),\n",
    "        'AUC': roc_auc_score(yt, y_score),\n",
    "        'F1': f1_score(yt, yp, zero_division=0),\n",
    "        'Precision': precision_score(yt, yp, zero_division=0),\n",
    "        'Recall': recall_score(yt, yp, zero_division=0),\n",
    "        'MCC': matthews_corrcoef(yt, yp),\n",
    "        'EOD': tpr(yt[unpriv], yp[unpriv]) - tpr(yt[priv], yp[priv]),\n",
    "        'AOD': 0.5 * ((fpr(yt[unpriv], yp[unpriv]) - fpr(yt[priv], yp[priv])) +\n",
    "                      (tpr(yt[unpriv], yp[unpriv]) - tpr(yt[priv], yp[priv]))),\n",
    "        'DI': sr_unpriv / sr_priv if sr_priv > 0 else np.nan,\n",
    "        'SPR': sr_unpriv - sr_priv\n",
    "    }\n",
    "\n",
    "metrics_qnn_before = full_metrics(y_test, y_pred_qnn, y_scores_qnn, sens_test)\n",
    "metrics_qnn_after = full_metrics(y_test, y_pred_qnn_fair, y_scores_qnn, sens_test)\n",
    "metrics_mlp_before = full_metrics(y_test, y_pred_mlp, y_scores_mlp, sens_test)\n",
    "metrics_mlp_after = full_metrics(y_test, y_pred_mlp_fair, y_scores_mlp, sens_test)\n",
    "metrics_logreg_before = full_metrics(y_test, y_pred_logreg, y_scores_logreg, sens_test)\n",
    "metrics_logreg_after = full_metrics(y_test, y_pred_logreg_fair, y_scores_logreg, sens_test)\n",
    "\n",
    "metrics_df = pd.DataFrame([\n",
    "    metrics_qnn_before,\n",
    "    metrics_qnn_after,\n",
    "    metrics_mlp_before,\n",
    "    metrics_mlp_after,\n",
    "    metrics_logreg_before,\n",
    "    metrics_logreg_after\n",
    "], index=[\"QNN Before\", \"QNN After\", \"MLP Before\", \"MLP After\", \"LogReg Before\", \"LogReg After\"])\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e46bab-4c86-47a7-b848-985206f2f26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairlearn in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from fairlearn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.3 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from fairlearn) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2.1 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from fairlearn) (1.7.0)\n",
      "Requirement already satisfied: scipy>=1.9.3 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from fairlearn) (1.15.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from scikit-learn>=1.2.1->fairlearn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/qiskit-env/lib/python3.10/site-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1495af4-2e86-443f-b4ab-68ce9f74e618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AdvEpoch 1] Loss: -54127.7640\n"
     ]
    }
   ],
   "source": [
    "# ... (tutto il codice precedente per COMPAS rimane invariato)\n",
    "\n",
    "# 6. Adversarial Debiasing (Quantum Predictor + Classico Adversary)\n",
    "\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class QAdvModel(nn.Module):\n",
    "    def __init__(self, q_model, adv_model, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.q_model = q_model\n",
    "        self.adv_model = adv_model\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        q_out = self.q_model(x)\n",
    "        adv_out = self.adv_model(q_out.detach())\n",
    "        return q_out, adv_out\n",
    "\n",
    "q_model = QuantumClassifier()\n",
    "adversary = Adversary(1)  # 1 output dalla QNN\n",
    "q_adv = QAdvModel(q_model, adversary, alpha=1.0)\n",
    "\n",
    "opt_pred = torch.optim.AdamW(q_adv.q_model.parameters(), lr=0.001)\n",
    "opt_adv = torch.optim.AdamW(q_adv.adv_model.parameters(), lr=0.001)\n",
    "\n",
    "sens_train_torch = torch.tensor(sens_train * 1.0, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch, s_batch in zip(X_train_torch, y_train_torch, sens_train_torch):\n",
    "        x_batch = x_batch.view(1, -1)\n",
    "        y_batch = y_batch.view(1, -1)\n",
    "        s_batch = s_batch.view(1, -1)\n",
    "\n",
    "        opt_pred.zero_grad()\n",
    "        opt_adv.zero_grad()\n",
    "\n",
    "        pred_out, adv_out = q_adv(x_batch)\n",
    "\n",
    "        pred_loss = loss_fn(pred_out, y_batch)\n",
    "        adv_loss = loss_fn(adv_out, s_batch)\n",
    "        loss = pred_loss - q_adv.alpha * adv_loss\n",
    "\n",
    "        loss.backward()\n",
    "        opt_pred.step()\n",
    "        opt_adv.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[AdvEpoch {epoch+1}] Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Valutazione del modello adversarial\n",
    "qadv_scores = q_model(X_test_torch).detach().numpy().flatten()\n",
    "qadv_pred = (qadv_scores >= 0.5).astype(int)\n",
    "\n",
    "metrics_qadv = full_metrics(y_test, qadv_pred, qadv_scores, sens_test)\n",
    "metrics_df.loc['QNN Adversarial'] = metrics_qadv\n",
    "\n",
    "# Equalized Odds Postprocessing su QNN Adversarial\n",
    "class QAdvProbModel(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        return np.vstack([1 - qadv_scores, qadv_scores]).T\n",
    "    def _check_is_fitted(self):\n",
    "        pass\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "post_qadv = ThresholdOptimizer(\n",
    "    estimator=QAdvProbModel(),\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True\n",
    ")\n",
    "post_qadv.fit(X=np.zeros_like(qadv_scores).reshape(-1, 1), y=y_test, sensitive_features=sens_test)\n",
    "qadv_fair = post_qadv.predict(X=np.zeros_like(qadv_scores).reshape(-1, 1), sensitive_features=sens_test)\n",
    "\n",
    "metrics_qadv_fair = full_metrics(y_test, qadv_fair, qadv_scores, sens_test)\n",
    "metrics_df.loc['QNN Adv + EO'] = metrics_qadv_fair\n",
    "\n",
    "print(\"\\n=== Confronto con QNN Adversarial Debiasing + Equalized Odds ===\")\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173047d6-fe4f-4309-9708-96ea08a2b960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qiskit-env)",
   "language": "python",
   "name": "qiskit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
